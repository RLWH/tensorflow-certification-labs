{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYOPzH3Kh1djqfGuPkAvNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/tensorflow-certification-labs/blob/main/C3_W1_Lab_2_sequences_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ungraded Lab: Generating Sequences and Padding\n",
        "In this lab, you will look at converting your input sentences into a sequence of tokens. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections."
      ],
      "metadata": {
        "id": "KaLGfLt74Tre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to sequence\n",
        "In the previous lab, we saw how to generate a `word_index` dictionary to generate tokens for each word in our corpus. \n",
        "\n",
        "We can then use the result to convert each of the input sentences into a sequence of tokens. This is done using the `texts_to_sequences()` method as shown below. "
      ],
      "metadata": {
        "id": "fHJii8SS4ZB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSya9UBh4LzO",
        "outputId": "ab978359-581b-4fa7-f036-8261bf3945a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define our input texts\n",
        "sentences = [\n",
        "    \"I love my dog\",\n",
        "    \"I love my cat\",\n",
        "    \"You love my dog!\",\n",
        "    \"Do you think my dog is amazing?\"\n",
        "]\n",
        "\n",
        "# Initialise the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"\")\n",
        "\n",
        "# Tokenise the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate list of okten sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nWord Index = \", word_index)\n",
        "print(\"\\nSequences = \", sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "We will usually need to pad the sequences into a uniform length because that is what our model expects. \n",
        "\n",
        "We can use the `pad_sequences` for that. By default, it will pad according to the length of the longest sequence. We can override this with the `maxlen` argument to define a specific length. "
      ],
      "metadata": {
        "id": "ctYJG2zG5i6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to a uniform length\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nPadded Sequences: \\n\", padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrqJxyWu5My5",
        "outputId": "e8c58701-da09-4970-9766-5acfbba35c4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Sequences: \n",
            " [[ 0  5  3  2  4]\n",
            " [ 0  5  3  2  7]\n",
            " [ 0  6  3  2  4]\n",
            " [ 9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out-of-vocabulary tokens\n",
        "Notice that we defined an `oov_token` when the `Tokenizer` was initialised earlier. This will be used when you have input words that are not found in the `word_index` dictionary. \n",
        "For example, we may decide to collect more text after our initial training and decide to not re-generate the `word_index`. We will see this in action in the cell below. \n",
        "\n",
        "Notice the token `1` is inserted for words that are not found in the dictionary"
      ],
      "metadata": {
        "id": "utsNSfjk6sAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "# Generate the sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Print the word index dictionary\n",
        "print(\"\\nWord Index = \", word_index)\n",
        "\n",
        "# Print the sequences with OOV\n",
        "print(\"\\nTest Sequences = \", test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AVSMcJK6CCm",
        "outputId": "796c5e25-692e-4f1f-c1ce-936ce43af038"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Test Sequences =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the padded result\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Y0WbT27Vvy",
        "outputId": "91f60af3-e3d0-422b-adf3-674e6c5fdd30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Test Sequence: \n",
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 0 0 2 4 1 2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmZxgoja7cqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}